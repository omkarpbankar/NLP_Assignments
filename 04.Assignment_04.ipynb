{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fc44854",
   "metadata": {},
   "source": [
    "# Assignment 04 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35eaef1",
   "metadata": {},
   "source": [
    "#### 1.\tCan you think of a few applications for a sequence-to-sequence RNN? What about a sequence-to-vector RNN? And a vector-to-sequence RNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115b7b58",
   "metadata": {},
   "source": [
    "![sequence-to-sequence RNN](https://maucher.home.hdm-stuttgart.de/Pics/many2manyLanguageModel.png)\n",
    "                           \n",
    "**Sequence-toSsequence RNN**takes a sequence of inputs and produce a sequence of outputs.\n",
    "\n",
    "**Applications**:\n",
    "- predicting stock prices\n",
    "- predicting weather\n",
    "- machine translation\n",
    "- video captioning\n",
    "- speech 2 text\n",
    "- music generation\n",
    "- identifying chords\n",
    "\n",
    "\n",
    "**Sequence-to-Vector RNN** feeds the network a sequence of inputs, and ignore all outputs except for the last one.** \n",
    "**Applications**:\n",
    "- analyzing sentiment of a review\n",
    "- classifying audio files into music genres\n",
    "- recommender systems.\n",
    "\n",
    "**Sequence-to-Vector RNN is above**\n",
    "\n",
    "![Sequence-to-Vector RNN](https://www.dinhanhthi.com/img/post/mooc/tf/rnn_ts_sequence_to_vector-640w.webp)\n",
    "\n",
    "**Vector-to-Sequence RNN** feed the network a single input at the first time step (and zeros for all other time steps), and let it output a sequence\n",
    "\n",
    "**Applications**:\n",
    "- captioning images\n",
    "- creating a playlist\n",
    "- generating a melody\n",
    "- locating pedestrians in a frame of video\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c478edb2",
   "metadata": {},
   "source": [
    "#### 2.\tWhy do people use encoder–decoder RNNs rather than plain sequence-to-sequence RNNs for automatic translation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fad69ca",
   "metadata": {},
   "source": [
    "**Seq-2-Seq RNNs model translate one word at a time whereas encoder-decoder RNNs read & translate a sentence at a time**\n",
    "\n",
    "The encoder-decoder architecture for recurrent neural networks is the standard neural machine translation method that rivals and in some cases outperforms classical statistical machine translation methods.\n",
    "\n",
    "![encoder-decoder architecture](https://machinelearningmastery.com/wp-content/uploads/2017/10/Depiction-of-Sutskever-Encoder-Decoder-Model-for-Text-Translation.png)\n",
    "**Encoder-Decoder Architecture**\n",
    "\n",
    "The Encoder-Decoder architecture with recurrent neural networks has become an effective and standard approach for both **neural machine translation (NMT) and sequence-to-sequence (seq2seq) prediction in general.**\n",
    "\n",
    "The key benefits of the approach are the ability to train a single end-to-end model directly on source and target sentences and the ability to handle variable length input and output sequences of text.\n",
    "\n",
    "An Encoder-Decoder architecture was developed where an input sequence was read in entirety and encoded to a fixed-length internal representation.\n",
    "\n",
    "A decoder network then used this internal representation to output words until the end of sequence token was reached. LSTM networks were used for both the encoder and decoder.\n",
    "\n",
    "Encoder–Decoder RNN consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb759d62",
   "metadata": {},
   "source": [
    "#### 3.\tHow could you combine a convolutional neural network with an RNN to classify videos?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8182104a",
   "metadata": {},
   "source": [
    "**To combine a convolutional neural network with an RNN to classify videos**\n",
    "- 1. Run a frame from each second of video through a CNN\n",
    "\n",
    "- 2. Feed CNN outputs as input sequence to RNN\n",
    "\n",
    "- 3. Feed RNN outputs to softmax layer for probabilities of each class\n",
    "\n",
    "Two approaches are used combine a convolutional neural network with an RNN to classify videos.\n",
    "- The first approach is described in the paper of  Andrej Karpathy and  Li Fei-Fei: They connect a CNN and RNN in series and use this for labeling a scene with a whole sentence (14). \n",
    "- The second approach from Ming Liang and Xiaolin Hu mixes a CNN with a RNN and use this architecture for better object detection (6).\n",
    " \n",
    "Most of these are readily identified as images that occur in a temporal sequence, in other words video.\n",
    "Each video is converted into sequential images and passed onto the CNN to extract spatial features. The outputs are then passed into a recurrent sequence learning model (i.e. LSTM) to identify temporal features within the image sequence.\n",
    "\n",
    "\n",
    "- CNNs are good with hierarchical or spatial data and extracting unlabeled features. Those could be images or written characters.  CNNs take fixed size inputs and generate fixed size outputs.\n",
    "- RNNs are good at temporal or otherwise sequential data. Could be letters or words in a body of text, stock market data, or speech recognition.  RNNs can input and output arbitrary lengths of data.  LSTMs are a variant of RNNs that allow for controlling how much of prior training data should be remembered, or more appropriately forgotten.\n",
    "\n",
    "Application of RNN and CNN\n",
    "- Video Scene Labeling\n",
    "- Emotion Detection\n",
    "- Video Based Person Re-identification / Gait Recognition\n",
    "- Weather Prediction\n",
    "- Creating Realistic Sound Tracks for Silent Videos\n",
    "\n",
    "![Image](https://wiki.tum.de/download/attachments/22578349/ExampleCNNRNN.JPG?version=1&modificationDate=1485387403397&api=v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a639b7b3",
   "metadata": {},
   "source": [
    "#### 4.\tWhat are the advantages of building an RNN using dynamic_rnn() rather than static_rnn()?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b6daeb",
   "metadata": {},
   "source": [
    "1. Avoids out-of-memory errors\n",
    "\n",
    "2. Directly takes single tensor as input and output (covering all time steps)\n",
    "\n",
    "3. No need to stack, unstack, or transpose\n",
    "\n",
    "4. Generates a smaller easier to visualize graph in TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4d0589",
   "metadata": {},
   "source": [
    "#### 5.\tHow can you deal with variable-length input sequences? What about variable-length output sequences?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b187f5",
   "metadata": {},
   "source": [
    "- To deal with variable-length input sequences: set sequence_length parameter when calling static_rnn() or dynamic_rnn()\n",
    "\n",
    "- To deal with variable-length output sequences: pad smaller input/output to make them same size as largest input/output\n",
    "\n",
    "The most commonly adopted solution is to truncate all inputs to the same length, which usually coincides with the shorter length input. However, this creates a huge loss of data, and as we know, data is gold to us.\n",
    "\n",
    "One possible alternative is its opposite, which is padding (add data until all signals are at the same length). The problem with padding is that it adds data with no real sense, and also with very long inputs, the network becomes unsustainable in size. Of course, padding could be done via augmentation. However, in particular as regards the signals in which the order of the data is fundamental, applying augmentation is going to “dirty” this information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fa3100",
   "metadata": {},
   "source": [
    "#### 6.\tWhat is a common way to distribute training and execution of a deep RNN across multiple GPUs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb16b1ff",
   "metadata": {},
   "source": [
    "Common way to distribute training and execution of a deep RNN across multiple GPUs is to **place each layer on a different GPU.**\n",
    "\n",
    "Strategy is a TensorFlow API to distribute training across multiple GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f63ea9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
